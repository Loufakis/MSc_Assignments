{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "838574aa",
   "metadata": {},
   "source": [
    "# Violence Against Women"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5c4d8aa2",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "1. [Set Up](#Set+Up)\n",
    "2. [Split Datasetd](#Split_Datasetd)\n",
    "3. [Remove Outliers](#Remove_Outliers)\n",
    "4. [Impute](#Impute)\n",
    "5. [Scale](#Scale)\n",
    "6. [P.C.A.](#P.C.A.)\n",
    "7. [Data Mining](#Data_Mining)\n",
    "    1. [Logistic Regression](#Logistic_Regression)\n",
    "    2. [k-Nearest Neighbors](#k-Nearest_Neighbors)\n",
    "    3. [Disicion Tree](#Disicion_Tree)\n",
    "    4. [Support Vector Machine](#Support_Vector_Machine)\n",
    "    5. [Random Forest](#Random_Forest)\n",
    "    6. [XGBoosting](#XGBoosting)\n",
    "8. [Evaluation-Interpretation](#Evaluation-Interpretation)\n",
    "9. [Prediction](#Prediction)\n",
    "    1. [Merge All Three SubDatasets Back to One](#Merge_All_Three_SubDatasets_Back_to_One)\n",
    "    2. [Save New Dataset as .csv](#Save_New_Dataset_as_.csv)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b23e4e19",
   "metadata": {},
   "source": [
    "<a id=\"Set_Up\"></a>\n",
    "## Set_Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda679ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "# Basics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Plots\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "\n",
    "# Preprocess\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Cross validation\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Evaluation\n",
    "from sklearn import metrics\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af3c263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Preprocessed Data from WAV_Preprocessed.csv file\n",
    "data = pd.read_csv('datasets/WAV_Preprocessed.csv', \n",
    "                    # Specify the first row as the header\n",
    "                    header=0\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6d3c8e",
   "metadata": {},
   "source": [
    "### Split_Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "686a61ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split sets X(train, test, predic) + y(train, test, predic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "da6093d8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (435, 68)\n",
      "y_train: (435,)\n",
      "X_test:  (109, 68)\n",
      "y_test:  (109,)\n",
      "X_pred: (1472, 68)\n",
      "y_pred: (1472,)\n"
     ]
    }
   ],
   "source": [
    "prediction_set = data.loc[data['Outcome'] == outcome_encoder['ANY']]\n",
    "train_set = data.loc[data['Outcome'] != outcome_encoder['ANY']]\n",
    "\n",
    "X_pred = prediction_set.drop('Outcome', axis=1)\n",
    "y_pred = prediction_set['Outcome']\n",
    "\n",
    "X = train_set.drop('Outcome', axis=1)\n",
    "y = train_set['Outcome']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#shape of dataset\n",
    "print('X_train: '  + str(X_train.shape))\n",
    "print('y_train: '  + str(y_train.shape))\n",
    "print('X_test:  '  + str(X_test.shape))\n",
    "print('y_test:  '  + str(y_test.shape))\n",
    "print('X_pred: '   + str(X_pred.shape))\n",
    "print('y_pred: '   + str(y_pred.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4953145f",
   "metadata": {},
   "source": [
    "### Remove_Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "3fb2bcc9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Observation_Value'>"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.boxplot(x=X_train['Observation_Value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "5bd3a814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range: 23.65\n",
      "Q1: 7.275\n",
      "Q3: 30.925\n"
     ]
    }
   ],
   "source": [
    "Q1 = X_train['Observation_Value'].quantile(0.25)\n",
    "Q3 = X_train['Observation_Value'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "print(f'Range: {IQR}')\n",
    "print(f'Q1: {Q1}')\n",
    "print(f'Q3: {Q3}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "3d5489ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bounds: [-28.199999999999996,66.39999999999999]\n",
      "Bounds Range: 94.6\n"
     ]
    }
   ],
   "source": [
    "lower_bound = Q1 - (1.5 * IQR) \n",
    "upper_bound = Q3 + (1.5 * IQR)\n",
    "\n",
    "print(f'Bounds: [{lower_bound},{upper_bound}]')\n",
    "print(f'Bounds Range: {upper_bound-lower_bound}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9ef17f",
   "metadata": {},
   "source": [
    "#### Locate the rows with outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "74709dcf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 6 outliers:\n",
      "[1720, 1866, 1865, 1551, 1089, 1344]\n"
     ]
    }
   ],
   "source": [
    "# outlier rows indexies\n",
    "outliers_rows = (X_train['Observation_Value'].index[(X_train['Observation_Value'] < lower_bound) |(X_train['Observation_Value'] > upper_bound)]).tolist()\n",
    "print(f'There are {len(outliers_rows)} outliers:')\n",
    "print(outliers_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "e2940176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the outliers\n",
    "X_train = X_train.drop(outliers_rows, axis=0)\n",
    "y_train = y_train.drop(outliers_rows, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "7d4ec60d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "429\n"
     ]
    }
   ],
   "source": [
    "# Check...\n",
    "print(X_train.shape[0] == y_train.shape[0])\n",
    "print(X_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "c45be30f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Observation_Value'>"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.boxplot(x=X_train['Observation_Value']) # seems better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "bfc83e34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x1e610cd7fa0>"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsgAAALICAYAAABiqwZ2AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAgbklEQVR4nO3de7TdZ13n8c+3PUVAqsAQWCVNpoKIIg4BYgdSL0jBVRlH6gwYWIrVYWzWDMwCL8xicGYU11x0Dd5GZzBVsB3FmiIgiK5irQhKEExrgXYK1kulobVNdSkoI5j2mT/OL/AlTdKT5Pz2zuX1Wmuvs/dvX57nPD1J3+uX5+xdY4wAAACrzlj2BAAA4EQikAEAoBHIAADQCGQAAGgEMgAANCvLnsBaXHTRRePqq69e9jQAADi11KEOnhRnkO++++5lTwEAgNPESRHIAACwKAIZAAAagQwAAI1ABgCARiADAEAzWyBX1QOr6v1V9YGquqmqXj0df3hVXVNVt0xfHzbXHAAA4GjNeQb5U0meOcZ4UpItSS6qqqcleWWSa8cYj0ty7XQbAABOCLMF8lj1t9PNs6bLSPLcJFdMx69IcvFccwAAgKM16x7kqjqzqm5IcleSa8YY70vyqDHGHUkyfX3kYZ57aVXtqao9+/btm3OaAADwGbMG8hjjnjHGliTnJjm/qp54FM+9bIyxdYyxdcOGDbPNEQAAuoW8i8UY46+T/E6Si5LcWVXnJMn09a5FzAEAANZiznex2FBVD52uPyjJs5J8OMnbklwyPeySJG+daw4AAHC0VmZ87XOSXFFVZ2Y1xK8aY7y9qt6b5KqqenGSjyZ5/oxzAACAozJbII8xPpjkyYc4/pdJLpxrXAAAOB4+SQ8AABqBDAAAjUAGAIBGIAMAQCOQAQCgEcgAANAIZAAAaAQyAAA0AhkAABqBDAAAjUAGAIBGIAMAQCOQAQCgEcgAANAIZAAAaAQyAAA0AvkINm7anKqa/bJx0+Zlf6sAAExWlj2BE9nte2/L9p27Zx9n145ts48BAMDaOIMMAACNQAYAgEYgAwBAI5ABAKARyAAA0AhkAABoBDIAADQCGQAAGoEMAACNQAYAgEYgAwBAI5ABAKARyAAA0AhkAABoBDIAADQCGQAAGoEMAACNQAYAgEYgAwBAI5ABAKARyAAA0AhkAABoBDIAADQCGQAAGoEMAACNQAYAgEYgAwBAI5ABAKARyAAA0AhkAABoBDIAADQCGQAAGoEMAACNQAYAgEYgAwBAI5ABAKARyAAA0AhkAABoBDIAADQCGQAAGoEMAACNQAYAgEYgAwBAI5ABAKARyAAA0AhkAABoBDIAADQCGQAAGoEMAACNQAYAgEYgAwBAI5ABAKARyAAA0AhkAABoBDIAADQCGQAAGoEMAACNQAYAgEYgAwBAI5ABAKARyAAA0AhkAABoBDIAADQCGQAAGoEMAACNQAYAgEYgAwBAI5ABAKARyAAA0AhkAABoBDIAADQCGQAAGoEMAACNQAYAgEYgAwBAI5ABAKARyAAA0AhkAABoBDIAADQCGQAAGoEMAACNQAYAgGa2QK6qTVX1zqq6uapuqqqXTcd/sKo+VlU3TJfnzDUHAAA4Wiszvvb+JN87xri+qs5Ocl1VXTPd9+NjjNfMODYAAByT2QJ5jHFHkjum65+oqpuTbJxrPAAAWA8L2YNcVecleXKS902HXlpVH6yq11fVwxYxBwAAWIvZA7mqHpLkTUlePsb4eJLXJnlski1ZPcP8o4d53qVVtaeq9uzbt2/uaQIAQJKZA7mqzspqHL9hjPHmJBlj3DnGuGeMcW+Sn01y/qGeO8a4bIyxdYyxdcOGDXNOEwAAPmPOd7GoJK9LcvMY48fa8XPaw745yY1zzQEAAI7WnO9icUGSFyX5UFXdMB17VZIXVtWWJCPJrUl2zDgHAAA4KnO+i8XvJalD3PUbc40JAADHyyfpAQBAI5ABAKARyAAA0AhkAABoBDIAADQCGQAAGoEMAACNQAYAgEYgAwBAI5ABAKARyAAA0AhkAABoBDIAADQCGQAAGoEMAACNQAYAgEYgAwBAI5ABAKARyAAA0AhkAABoBDIAADQCGQAAGoEMAACNQAYAgEYgAwBAI5ABAKARyAAA0AhkAABoBDIAADQCGQAAGoEMAACNQAYAgEYgAwBAI5ABAKARyAAA0AhkAABoBDIAADQCGQAAGoEMAACNQAYAgEYgAwBAI5ABAKARyAAA0AhkAABoBDIAADQCGQAAGoEMAACNQAYAgEYgAwBAI5ABAKARyAAA0AhkAABoBDIAADQCGQAAGoEMAACNQAYAgEYgAwBAI5ABAKARyAAA0AhkAABoBPJpZuOmzamq2S8bN20+pb6fRX5PAMByrSx7AizW7Xtvy/adu2cfZ9eObbOPkSzu+0kW9z0BAMvlDDIAADQCGQAAGoEMAACNQAYAgEYgAwBAI5ABAKARyAAA0AhkAABoBDIAADQCGQAAGoEMAACNQAYAgEYgAwBAI5ABAKARyAAA0AhkAABoBDIAADQCGQAAGoEMAACNQAYAgGZl2RMgyRkrqaplzwIAgAjkE8O9+7N95+6FDLVrx7aFjAMAcLKyxQIAABqBDAAAjUAGAIBGIAMAQCOQAQCgEcgAANAIZAAAaAQyAAA0AhkAABqBDAAAjUAGAIBGIAMAQCOQAQCgEcgAANAIZAAAaAQyAAA0AhkAAJrZArmqNlXVO6vq5qq6qapeNh1/eFVdU1W3TF8fNtccAADgaM15Bnl/ku8dY3xZkqcleUlVPSHJK5NcO8Z4XJJrp9sAAHBCmC2Qxxh3jDGun65/IsnNSTYmeW6SK6aHXZHk4rnmAAAAR2she5Cr6rwkT07yviSPGmPckaxGdJJHHuY5l1bVnqras2/fvkVMEwAA5g/kqnpIkjclefkY4+Nrfd4Y47IxxtYxxtYNGzbMN0EAAGhmDeSqOiurcfyGMcabp8N3VtU50/3nJLlrzjkAAMDRmPNdLCrJ65LcPMb4sXbX25JcMl2/JMlb55oDAAAcrZUZX/uCJC9K8qGqumE69qokP5zkqqp6cZKPJnn+jHMAAICjMlsgjzF+L0kd5u4L5xoXAACOh0/SAwCARiADAEAjkAEAoBHIAADQCGQAAGgEMgAANAIZAAAagQwAAI1ABgCARiADAEAjkAEAoBHIAADQCGQAAGgEMgAANAIZAAAagQwAAM3KsifAKeqMlVTVsmcBAHDUBDLzuHd/tu/cPfswu3Zsm30MAOD0YosFAAA0AhkAABqBDAAAjUAGAIBGIAMAQCOQAQCgEcgAANAIZAAAaAQyAAA0AhkAABqBDAAAjUAGAIBGIAMAQCOQAQCgEcgAANAIZAAAaAQyAAA0AhkAABqBDAAAjUAGAIBGIAMAQCOQAQCgEcgAANAIZAAAaAQyAAA0AhkAABqBDAAAjUAGAIBGIAMAQCOQAQCgEcgAANAIZAAAaAQyAAA0AhkAABqBDAAAjUAGAIBGIAMAQCOQAQCgEcgAANAIZAAAaAQyAAA0AhkAABqBDCeYjZs2p6pmv2zctHnZ3yoAnJBWlj0B4HPdvve2bN+5e/Zxdu3YNvsYAHAycgYZAAAagQwAAI1ABgCARiADAEAjkAEAoBHIAADQCGQAAGgEMgAANAIZAAAagQwAAI1ABgCARiADAEAjkAEAoBHIAADQCGQAAGgEMgAANAIZAAAagQwAAI1ABgCARiADAEAjkAEAoBHIAADQCGQAAGgEMgAANGsK5Kq6YC3HAADgZLfWM8g/tcZjAABwUls50p1V9fQk25JsqKrvaXd9QZIz55wYAAAswxEDOckDkjxketzZ7fjHkzxvrkkBAMCyHDGQxxjvSvKuqrp8jPHnC5oTAAAszf2dQT7g86rqsiTn9eeMMZ45x6QAAGBZ1hrIb0zyM0l+Lsk9800HAACWa62BvH+M8dpZZwIAACeAtb7N269V1b+tqnOq6uEHLrPODAAAlmCtZ5Avmb6+oh0bSR6zvtMBAIDlWlMgjzG+aO6JAADAiWBNgVxV336o42OM/7O+0wEAgOVa6xaLr2zXH5jkwiTXJxHIAACcUta6xeLf9dtV9YVJfmGWGQEAwBKt9V0sDvbJJI870gOq6vVVdVdV3diO/WBVfayqbpguzznG8QEAYBZr3YP8a1l914okOTPJlyW56n6ednmSn859t2H8+BjjNUcxRwAAWJi17kHuQbs/yZ+PMfYe6QljjHdX1XnHOjEAAFiGNW2xGGO8K8mHk5yd5GFJPn0cY760qj44bcF42HG8DgAArLs1BXJVfUuS9yd5fpJvSfK+qnreMYz32iSPTbIlyR1JfvQIY15aVXuqas++ffuOYSgAADh6a91i8f1JvnKMcVeSVNWGJL+V5FeOZrAxxp0HrlfVzyZ5+xEee1mSy5Jk69at43CPAwCA9bTWd7E440AcT/7yKJ77GVV1Trv5zUluPNxjAQBgGdZ6BvnqqnpHkiun29uT/MaRnlBVVyZ5RpJHVNXeJD+Q5BlVtSWr74hxa5IdRz9lAACYzxEDuaq+OMmjxhivqKp/keSrklSS9yZ5w5GeO8Z44SEOv+5YJwoAAItwf9skfiLJJ5JkjPHmMcb3jDG+O6tnj39i3qkBAMDi3V8gnzfG+ODBB8cYe5KcN8uMAABgie4vkB94hPsetJ4TAQCAE8H9BfIfVNV3HXywql6c5Lp5pgQAAMtzf+9i8fIkb6mqb81ng3hrkgdk9W3aAADglHLEQJ4+2GNbVX1dkidOh399jPHbs88MAACWYE3vgzzGeGeSd848FwAAWLqj/jQ8AAA4lQlkAABoBDIAADQCGQAAGoEMAACNQAYAgEYgAwBAI5ABAKARyAAA0AhkAABoBDIAADQCGQAAGoEMAACNQAYAgEYgAwBAI5ABAKARyAAA0AhkAABoBDIAADQCGQAAGoEMAACNQAYAgEYgAwBAI5ABAKARyAAA0AhkAABoBDIAADQCGQAAGoEMAACNQAYAgEYgAwBAI5ABAKARyAAA0AhkAABoBDIAADQCGQAAGoEMAACNQAYAgEYgAwBAI5ABAKARyAAA0AhkAABoBDIAADQCGQAAGoEMAACNQAYAgEYgAwBAI5ABAKARyAAA0AhkAABoBDIAADQCGQAAGoEMAACNQIbT1RkrqarZLxs3bV72dwoAR2Vl2RMAluTe/dm+c/fsw+zasW32MQBgPTmDDAAAjUAGAIBGIAMAQCOQAQCgEcgAANAIZAAAaAQyAAA0AhkAABqBDAAAjUAGAIBGIAMAQCOQAQCgEcgAANAIZAAAaAQyAAA0AhkAABqBDAAAjUAGAIBGIAMAQCOQAQCgEciwVmespKpmv3Di27hp80J+FjZu2rzsbxXgtLSy7AnASePe/dm+c/fsw+zasW32MTg+t++9zc8CwCnMGWQAAGgEMgAANAIZAAAagQwAAI1ABgCARiADAEAjkAEAoBHIAADQCGQAAGgEMgAANAIZAAAagQwAAI1ABgCARiADAEAjkAEAoBHIAADQCGQAAGhmC+Sqen1V3VVVN7ZjD6+qa6rqlunrw+YaHwAAjsWcZ5AvT3LRQcdemeTaMcbjklw73QYAgBPGbIE8xnh3kr866PBzk1wxXb8iycVzjQ8AAMdi0XuQHzXGuCNJpq+PPNwDq+rSqtpTVXv27du3sAkCAHB6O2F/SW+McdkYY+sYY+uGDRuWPR0AAE4Tiw7kO6vqnCSZvt614PEBAOCIFh3Ib0tyyXT9kiRvXfD4AABwRHO+zduVSd6b5PFVtbeqXpzkh5M8u6puSfLs6TYAAJwwVuZ64THGCw9z14VzjQkAAMfrhP0lPQAAWAaBDAAAjUAGAIBGIAMAQCOQAQCgEcgAANAIZAAAaAQyAAA0AhkAABqBDAAAjUAGAIBGIAMAQCOQAQCgEcgAANAIZAAAaAQyAAA0K8ueAHCKO2MlVbWQoc486/Nyzz98aiFjAXDqEsjAvO7dn+07dy9kqF07ti1krF07ts0+BgDLY4sFAAA0AhkAABqBDAAAjUAGAIBGIAMAQCOQAQCgEcgAANAIZAAAaAQyAAA0AhkAABqBDAAAjUAGAIBGIAMAQCOQAQCgEcgAANAIZAAAaAQyAAA0AhkAABqBDAAAjUAGAIBGIAPAMdq4aXOqavbLxk2bl/2twmllZdkTAICT1e17b8v2nbtnH2fXjm2zjwF8ljPIAADQCGQAAGgEMgAANAIZAAAagQwAAI1ABgCARiADAEAjkAEAoBHIAADQCGQAAGgEMgAANAIZAAAagQwAAI1ABgCARiADAEAjkAEAoBHIAADQCGQAAGgEMgAANAIZAAAagQwAAI1ABgCARiADAEAjkAEAoBHIAADQCGQAAGgEMgAANAIZAAAagQwAAI1ABgCARiADAEAjkAEAoBHIAADQCGQAAGgEMgAANAIZAAAagQwAAI1ABgCARiADAEAjkAEAoBHIAADQCGQAAGgEMgCnnI2bNqeqZr8Ap6aVZU8AANbb7Xtvy/adu2cfZ9eObbOPASyeM8gAANAIZAAAaAQyAAA0AhkAABqBDAAAjUAGAIBGIAMAQCOQAQCgEcgAANAIZAAAaAQyAAA0AhkAABqBDAAAjUAGAIBGIAMAQCOQAQCgWVnGoFV1a5JPJLknyf4xxtZlzAMAAA62lECefN0Y4+4ljg8AAPdhiwUAADTLCuSR5Der6rqquvRQD6iqS6tqT1Xt2bdv34KnB3D62Lhpc6pqIZeNmzYv+9vlBLGonzs/cxyLZW2xuGCMcXtVPTLJNVX14THGu/sDxhiXJbksSbZu3TqWMUmA08Hte2/L9p27FzLWrh3bFjIOJ75F/dz5meNYLOUM8hjj9unrXUnekuT8ZcwDAAAOtvBArqrPr6qzD1xP8vVJblz0PAAA4FCWscXiUUneUlUHxv+lMcbVS5gHAADcx8IDeYzxp0metOhxAQBgLbzNGwAANAIZAAAagQwAAI1ABgCARiADAEAjkAEAoBHIAADQCGQAAGgEMgAANAIZAAAagQwAAI1ABgCARiADAEAjkAEAoBHIAADQCGQAAGhWlj0BAE4jZ6ykqpY9i5PPAtft0eduysdu++hCxoITlUAGYHHu3Z/tO3fPPsyuHdtmH2OhFrRuySm4dnAMbLEAAIBGIAMAQCOQAQCgEcgAANAIZAAAaAQyAAA0AhkAABqBDAAAjUAGAIBGIAMAQCOQAQCgEcgAANAIZAAAaAQyAAA0AhkAABqBDAAAjUAGAIBGIAMAQCOQAQCgEcgAANCsLHsCABzGGSupqmXPAk5uC/xz9OhzN+Vjt310IWMxL4EMcKK6d3+279w9+zC7dmybfQxYmgX9OUr8WTqV2GIBAACNQAYAgEYgAwBAI5ABAKARyAAA0AhkAABoBDIAADQCGQAAGoEMAACNQAYAgEYgAwBAI5ABAKARyAAA0AhkAABoBDIAADQCGQAAGoEMAACNQAYAgEYgAwBAI5ABAKARyADAZ52xkqqa/cKx27hp80L+G23ctHnZ3+rSrCx7AgDACeTe/dm+c/fsw+zasW32MU5Vt++9zX+jmTmDDAAAjUAGAIBGIAMAQCOQAQCgEcgAANAIZAAAaAQyAAA0AhkAABqBDAAAjUAGAIBGIAMAQCOQAQCgEcgAANAIZAAAaAQyAAA0AhkAABqBDAAAjUAGAIBGIAMAQCOQAQCgWVn2BAAATglnrKSqlj2L9bOg7+fR527Kx2776OzjHA2BDACwHu7dn+07d88+zK4d22YfI8mp9/0cBVssAACgEcgAANAIZAAAaAQyAAA0AhkAABqBDAAAjUAGAIBGIAMAQCOQAQCgEcgAANAIZAAAaAQyAAA0AhkAABqBDAAAjUAGAIBGIAMAQCOQAQCgWUogV9VFVfWRqvrjqnrlMuYAAACHsvBArqozk/yvJN+Q5AlJXlhVT1j0PAAA4FCWcQb5/CR/PMb40zHGp5P8cpLnLmEeAABwHzXGWOyAVc9LctEY419Pt1+U5J+OMV560OMuTXLpdPPxST6y0ImuekSSu5cw7unC+s7H2s7L+s7H2s7L+s7L+s5nrrW9e4xx0cEHV2YY6P7UIY7dp9LHGJcluWz+6RxeVe0ZY2xd5hxOZdZ3PtZ2XtZ3PtZ2XtZ3XtZ3Pote22VssdibZFO7fW6S25cwDwAAuI9lBPIfJHlcVX1RVT0gyQuSvG0J8wAAgPtY+BaLMcb+qnppknckOTPJ68cYNy16Hmu01C0epwHrOx9rOy/rOx9rOy/rOy/rO5+Fru3Cf0kPAABOZD5JDwAAGoEMAACNQD4MH4e9fqrq9VV1V1Xd2I49vKquqapbpq8PW+YcT2ZVtamq3llVN1fVTVX1sum4NT5OVfXAqnp/VX1gWttXT8et7TqqqjOr6g+r6u3Tbeu7Dqrq1qr6UFXdUFV7pmPWdp1U1UOr6leq6sPT379Pt77ro6oeP/3cHrh8vKpevsj1FciH4OOw193lSQ5+E+5XJrl2jPG4JNdOtzk2+5N87xjjy5I8LclLpp9Xa3z8PpXkmWOMJyXZkuSiqnparO16e1mSm9tt67t+vm6MsaW9f6y1XT8/meTqMcaXJnlSVn+Gre86GGN8ZPq53ZLkqUk+meQtWeD6CuRD83HY62iM8e4kf3XQ4ecmuWK6fkWSixc5p1PJGOOOMcb10/VPZPUv6Y2xxsdtrPrb6eZZ02XE2q6bqjo3yT9L8nPtsPWdj7VdB1X1BUm+JsnrkmSM8ekxxl/H+s7hwiR/Msb48yxwfQXyoW1Mclu7vXc6xvp51BjjjmQ18JI8csnzOSVU1XlJnpzkfbHG62L65/8bktyV5JoxhrVdXz+R5N8nubcds77rYyT5zaq6rqounY5Z2/XxmCT7kvz8tD3o56rq82N95/CCJFdO1xe2vgL50Nb0cdhwIqmqhyR5U5KXjzE+vuz5nCrGGPdM/8x3bpLzq+qJS57SKaOqvjHJXWOM65Y9l1PUBWOMp2R1u+BLquprlj2hU8hKkqckee0Y48lJ/i62U6y76QPlvinJGxc9tkA+NB+HPb87q+qcJJm+3rXk+ZzUquqsrMbxG8YYb54OW+N1NP3z6e9kdT+9tV0fFyT5pqq6Natb2Z5ZVb8Y67suxhi3T1/vyur+zfNjbdfL3iR7p39RSpJfyWowW9/19Q1Jrh9j3DndXtj6CuRD83HY83tbkkum65ckeesS53JSq6rK6j64m8cYP9bussbHqao2VNVDp+sPSvKsJB+OtV0XY4z/MMY4d4xxXlb/nv3tMca3xfoet6r6/Ko6+8D1JF+f5MZY23UxxviLJLdV1eOnQxcm+b+xvuvthfns9opkgevrk/QOo6qek9W9cQc+Dvu/LndGJ6+qujLJM5I8IsmdSX4gya8muSrJ5iQfTfL8McbBv8jHGlTVVyX53SQfymf3cb4qq/uQrfFxqKp/ktVfBDkzqycUrhpj/FBV/aNY23VVVc9I8n1jjG+0vsevqh6T1bPGyep2gF8aY/xXa7t+qmpLVn+59AFJ/jTJd2b6eyLW97hV1YOz+vtgjxlj/M10bGE/vwIZAAAaWywAAKARyAAA0AhkAABoBDIAADQCGQAAGoEMAACNQAY4jKo6t6reWlW3VNWfVNVPVtUDquo7quqnT4D5XVxVT2i3f6iqnrVOr31eVe2tqjMOOn5DVZ1/hOfcuB7jAyyTQAY4hOkTCt+c5FfHGI9L8iVJHpJklg8NqqqVY3jaxUk+E8hjjP88xvit9ZjPGOPWrL5J/1cfOFZVX5rk7DHG+9djDIATlUAGOLRnJvn7McbPJ8kY454k353kXyV5cJJNVXV1VX2kqn4g+czH+/56VX2gqm6squ3T8adW1buq6rqqekdVnTMd/52q+m9V9a4k319Vtx44Y1tVD66q26rqrKr6rqr6g+l13zTdty3JNyX5H9NZ3cdW1eVV9bzp+RdW1R9W1Yeq6vVV9XnT8Vur6tVVdf1035ceYQ2uzOpHQB/wgiRXTmeKf3d6jeunuXyOg8+yV9Xbp0/LS1V9fVW9d3ruG6vqIUf7HwdgTgIZ4NC+PMl1/cAY4+NZ/XjTlSTnJ/nWJFuSPL+qtia5KMntY4wnjTGemOTqqjoryU8led4Y46lJXp/PPQv90DHG144xXp3kA0m+djr+z5O8Y4zxD0nePMb4yjHGk5LcnOTFY4zdSd6W5BVjjC1jjD858IJV9cAklyfZPsb4imm+/6aNefcY4ylJXpvk+46wBlclubid3d6e5JeT3JXk2dNrbE/yP4/wGp+jqh6R5D8medb0/D1JvmetzwdYBIEMcGiVZBzh+DVjjL8cY/y/rG7F+KokH0ryrKr6kar66jHG3yR5fJInJrmmqm7Iahye215v10HXt0/XX9Due+J0xvZDWY3yL7+fuT8+yZ+NMf5oun1Fkq9p9795+npdkvMO9yJjjL9IclOSC6tqS5J/GGPcmOSsJD87zeeNads81uBp0+PfM63HJUn+8VE8H2B2x7LnDeB0cFOSf9kPVNUXJNmU5J7cN57HGOOPquqpSZ6T5L9X1W8meUuSm8YYTz/MOH/Xrr9tet7Dkzw1yW9Pxy9PcvEY4wNV9R1JnnE/c6/7uf9T09d7cv//HziwzeLO6XqyutXkziRPyuqJlr8/xPP253NPwjywze2aMcYL72dcgKVxBhng0K5N8uCq+vYkqaozk/xoVmP1k0meXVUPr6oHZfWX5d5TVY9O8skxxi8meU2SpyT5SJINVfX06XXOqqpDngEeY/xtkvcn+ckkb5/2PSfJ2UnumLZrfGt7yiem+w724STnVdUXT7dflORdR78ESZI3ZTX4D2yvSJIvTHLHGOPe6bXPPMTzbk2yparOqKpNWd2SkiS/n+SCA3Ob9lN/yTHODWAWAhngEMYYI8k3Z3V/8S1J/iirZ0pfNT3k95L8QpIbkrxpjLEnyVckef+0deD7k/yXMcankzwvyY9U1Qemx9/nl9qaXUm+LZ+79eI/JXlfkmuyGr8H/HKSV0y/jPfYNve/T/KdSd44bYO4N8nPHOUSHHitv85q1N45xviz6fD/TnJJVf1+Vt/d4+8O8dT3JPmzrG47eU2S66fX25fkO7L6y34fnF77SL8oCLBwtfr/AAAAIHEGGQAAPodf0gM4zVXVdyZ52UGH3zPGeMky5gOwbLZYAABAY4sFAAA0AhkAABqBDAAAjUAGAIDm/wOStzbLOq5sPQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.displot(X_train, x='Observation_Value', binwidth=3, height=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01df21b9",
   "metadata": {},
   "source": [
    "### Impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "e34048eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define imputer\n",
    "imp = IterativeImputer(max_iter=10, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "f3c26b52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fit and transform on train set\n",
    "imputed_array = imp.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "e517cc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Dataframe again\n",
    "X_train = pd.DataFrame(imputed_array, index=X_train.index, columns=X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "0dcd7e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform test and predict set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "d78362ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test set\n",
    "imputed_array = imp.transform(X_test)\n",
    "X_test = pd.DataFrame(imputed_array, index=X_test.index, columns=X_test.columns)\n",
    "\n",
    "# predict set\n",
    "imputed_array = imp.transform(X_pred)\n",
    "X_pred = pd.DataFrame(imputed_array, index=X_pred.index, columns=X_pred.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5c886f",
   "metadata": {},
   "source": [
    "### Scale "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "8d08f0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define min max scalers\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit and transform training data\n",
    "scaled_array = scaler.fit_transform(X_train)\n",
    "X_train = pd.DataFrame(scaled_array, index=X_train.index, columns=X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "7345a5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform test and predict set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "21348305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test set\n",
    "scaled_array = scaler.transform(X_test)\n",
    "X_test = pd.DataFrame(scaled_array, index=X_test.index, columns=X_test.columns)\n",
    "\n",
    "# predict set\n",
    "scaled_array = scaler.transform(X_pred)\n",
    "X_pred = pd.DataFrame(scaled_array, index=X_pred.index, columns=X_pred.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f93232",
   "metadata": {},
   "source": [
    "### P.C.A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "f49fa975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define PCA\n",
    "pca = PCA()\n",
    "pca = pca.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "f9a123b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Determine explained variance using explained_variance_ration_ attribute\n",
    "exp_var_pca = pca.explained_variance_ratio_\n",
    "#\n",
    "# Cumulative sum of eigenvalues; This will be used to create step plot\n",
    "# for visualizing the variance explained by each principal component.\n",
    "#\n",
    "cum_sum_eigenvalues = np.cumsum(exp_var_pca)\n",
    "#\n",
    "# Create the visualization plot\n",
    "#\n",
    "plt.bar(range(0,len(exp_var_pca)), exp_var_pca, alpha=0.5, align='center', label='Individual explained variance')\n",
    "plt.step(range(0,len(cum_sum_eigenvalues)), cum_sum_eigenvalues, where='mid',label='Cumulative explained variance')\n",
    "plt.ylabel('Explained variance ratio')\n",
    "plt.xlabel('Principal component index')\n",
    "plt.legend(loc='best')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "a8f43168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For 21 components there is over 99% of information kept!\n"
     ]
    }
   ],
   "source": [
    "total_var = 0\n",
    "\n",
    "for i, var in enumerate(exp_var_pca):\n",
    "    total_var += var\n",
    "    if total_var > .99:\n",
    "        print(f'For {i} components there is over 99% of information kept!')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "7362fa61",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "3f675030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "42ad3491",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_array = pca.fit_transform(X_train)\n",
    "X_train = pd.DataFrame(pca_array, index=X_train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "e14b7697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform test and predict set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "7afc1a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test set\n",
    "pca_array = pca.transform(X_test)\n",
    "X_test = pd.DataFrame(pca_array, index=X_test.index)\n",
    "\n",
    "# predict set\n",
    "pca_array = pca.transform(X_pred)\n",
    "X_pred = pd.DataFrame(pca_array, index=X_pred.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557afc61",
   "metadata": {},
   "source": [
    "### Data_Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab1113d",
   "metadata": {},
   "source": [
    "#### Logistic_Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "5a226bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.6, 'penalty': 'l2'}\n",
      "0.22159468438538205\n",
      "Wall time: 708 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Define the multinomial logistic regression model\n",
    "logisticRegressor = LogisticRegression(multi_class='multinomial',\n",
    "                                       random_state=96,\n",
    "                                       class_weight='balanced')\n",
    "\n",
    "# Define model parameters for the grid\n",
    "param_grid = {'penalty':['l1', 'l2'],\n",
    "              'C': [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]}\n",
    "\n",
    "# Define the (10-Fold) Cross Validation Grid Search\n",
    "search = GridSearchCV(estimator=logisticRegressor,\n",
    "                      param_grid=param_grid,\n",
    "                      scoring='accuracy',\n",
    "                      cv=10)\n",
    "\n",
    "# Lets search\n",
    "search.fit(X_train,\n",
    "           y_train)\n",
    "\n",
    "print(search.best_params_)\n",
    "print(search.best_score_)\n",
    "\n",
    "logisticRegressor = search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa9f3f0",
   "metadata": {},
   "source": [
    "### k-Nearest_Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "692386ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_neighbors': 32, 'p': 6, 'weights': 'uniform'}\n",
      "0.3030454042081949\n",
      "Wall time: 1min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Define the multinomial logistic regression model\n",
    "kNearestNeighbors = KNeighborsClassifier(metric='minkowski')\n",
    "\n",
    "# Define model parameters for the grid\n",
    "param_grid = {'n_neighbors':list(range(5,35)), # sqrt(435)~20 => [20-15,20+15] = [5,35]\n",
    "              'weights':['distance','uniform'],\n",
    "              'p':[1,2,3,4,5,6,7,8,9,10]}\n",
    "\n",
    "# Define the (10-Fold) Cross Validation Grid Search\n",
    "search = GridSearchCV(estimator=kNearestNeighbors,\n",
    "                      param_grid=param_grid,\n",
    "                      scoring='accuracy',\n",
    "                      cv=10)\n",
    "\n",
    "# Lets search\n",
    "search.fit(X_train,\n",
    "           y_train)\n",
    "\n",
    "print(search.best_params_)\n",
    "print(search.best_score_)\n",
    "\n",
    "kNearestNeighbors = search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37ecacf",
   "metadata": {},
   "source": [
    "### Disicion_Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "991ebedc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'criterion': 'gini', 'max_depth': 4, 'min_samples_leaf': 5, 'min_samples_split': 2, 'splitter': 'random'}\n",
      "0.3218715393133998\n",
      "Wall time: 21.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Define the multinomial logistic regression model\n",
    "decisionTree = DecisionTreeClassifier(random_state=96,\n",
    "                                      class_weight='balanced')\n",
    "\n",
    "# Define model parameters for the grid\n",
    "param_grid = {'criterion':['gini','entropy'],\n",
    "              'splitter':['best','random'],\n",
    "              'max_depth':[2,3,4,5,6,7,8],\n",
    "              'min_samples_split':[2,3,4],\n",
    "              'min_samples_leaf':[2,3,4,5,6]}\n",
    "\n",
    "# Define the (10-Fold) Cross Validation Grid Search\n",
    "search = GridSearchCV(estimator=decisionTree,\n",
    "                      param_grid=param_grid,\n",
    "                      scoring='accuracy',\n",
    "                      cv=10)\n",
    "\n",
    "# Lets search\n",
    "search.fit(X_train,\n",
    "           y_train)\n",
    "\n",
    "print(search.best_params_)\n",
    "print(search.best_score_)\n",
    "\n",
    "decisionTree = search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2b66aa",
   "metadata": {},
   "source": [
    "### Support_Vector_Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "87c7d39e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.125, 'kernel': 'linear'}\n",
      "0.33322259136212623\n",
      "Wall time: 6min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Define the multinomial logistic regression model\n",
    "supportVectorMachine = SVC(random_state=96,\n",
    "                           class_weight='balanced')\n",
    "\n",
    "# Define model parameters for the grid\n",
    "param_grid = [{'kernel':['linear'],\n",
    "               'C':[0.125, 0.5, 1, 2, 4, 8, 16, 32, 64, 128, 256, 512]},\n",
    "              {'kernel':['poly'],\n",
    "               'degree':[2,3,4,5,6,7,8],\n",
    "               'coef0':[0,1],\n",
    "               'gamma':[0.00390625, 0.0078125, 0.015625, 0.03125, 0.0625, 0.125, 0.5, 1, 2, 4],\n",
    "               'C':[0.125, 0.5, 1, 2, 4, 8, 16, 32, 64, 128, 256, 512]},\n",
    "              {'kernel':['rbf'],\n",
    "               'gamma':[0.00390625, 0.0078125, 0.015625, 0.03125, 0.0625, 0.125, 0.5, 1, 2, 4],\n",
    "               'C':[0.125, 0.5, 1, 2, 4, 8, 16, 32, 64, 128, 256, 512]},\n",
    "              {'kernel':['sigmoid'],\n",
    "               'coef0':[0,1],\n",
    "               'gamma':[0.00390625, 0.0078125, 0.015625, 0.03125, 0.0625, 0.125, 0.5, 1, 2, 4],\n",
    "               'C':[0.125, 0.5, 1, 2, 4, 8, 16, 32, 64, 128, 256, 512]},]\n",
    "\n",
    "# Define the (10-Fold) Cross Validation Grid Search\n",
    "search = GridSearchCV(estimator=supportVectorMachine,\n",
    "                      param_grid=param_grid,\n",
    "                      scoring='accuracy',\n",
    "                      cv=10)\n",
    "\n",
    "# Lets search\n",
    "search.fit(X_train,\n",
    "           y_train)\n",
    "\n",
    "print(search.best_params_)\n",
    "print(search.best_score_)\n",
    "\n",
    "supportVectorMachine = search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f679ee69",
   "metadata": {},
   "source": [
    "### Random_Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "03a816c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'criterion': 'entropy', 'max_depth': 3, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 20}\n",
      "0.28211517165005534\n",
      "Wall time: 7min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Define the multinomial logistic regression model\n",
    "randomForest = RandomForestClassifier(random_state=96,\n",
    "                                      class_weight='balanced')\n",
    "\n",
    "# Define model parameters for the grid\n",
    "param_grid = {'n_estimators':[5,10,15,20,25,30,35,40],\n",
    "              'criterion':['gini','entropy'],\n",
    "              'max_depth':[3,4,5,6,7,8],\n",
    "              'min_samples_split':[2,3,4,5],\n",
    "              'min_samples_leaf':[1,2,3]}\n",
    "\n",
    "# Define the (10-Fold) Cross Validation Grid Search\n",
    "search = GridSearchCV(estimator=randomForest,\n",
    "                      param_grid=param_grid,\n",
    "                      scoring='accuracy',\n",
    "                      cv=10)\n",
    "\n",
    "# Lets search\n",
    "search.fit(X_train,\n",
    "           y_train)\n",
    "\n",
    "print(search.best_params_)\n",
    "print(search.best_score_)\n",
    "\n",
    "randomForest = search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8feb03b",
   "metadata": {},
   "source": [
    "### XGBoosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "9b9ad090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:56:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:56:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:56:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:56:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:56:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:56:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:56:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:56:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:56:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:56:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:56:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:56:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:56:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:56:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:56:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:56:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:56:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:56:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:56:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:56:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:56:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:56:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:56:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:56:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:56:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:56:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:56:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:56:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:56:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:56:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:56:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:56:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:56:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:56:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:56:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:56:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:56:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:56:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:56:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:56:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:56:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:56:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:56:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:56:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:56:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:56:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:56:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:56:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:56:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:56:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:56:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:56:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:56:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:56:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:56:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:56:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:56:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:56:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:56:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:56:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:56:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:56:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:56:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:56:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:56:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:56:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:56:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:56:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:56:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:56:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:56:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:56:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:56:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:56:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:56:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:56:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:56:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:56:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:56:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:56:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:56:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:56:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:56:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:56:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:57:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:57:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:57:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:57:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:57:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:57:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:57:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:57:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:57:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:57:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:57:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:57:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:57:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:57:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:57:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:57:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:57:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:57:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:57:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:57:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:57:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:57:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:57:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:57:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:57:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:57:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:57:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:57:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:57:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:57:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:57:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:57:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:57:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:57:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:57:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:57:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:57:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:57:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:57:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:57:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:57:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:57:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:57:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:57:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:57:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:57:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:57:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:57:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:57:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:57:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:57:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:57:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:57:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:57:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:57:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:57:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:57:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:57:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "{'n_estimators': 125}\n",
      "0.2261904761904762\n",
      "Wall time: 56.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Define the multinomial logistic regression model\n",
    "xGBoost = XGBClassifier(random_state=96,\n",
    "                        class_weight='balanced')\n",
    "\n",
    "# Define model parameters for the grid\n",
    "param_grid = {'n_estimators':[125,130,135,140,145,150,155]}\n",
    "\n",
    "# Define the (10-Fold) Cross Validation Grid Search\n",
    "search = GridSearchCV(estimator=xGBoost,\n",
    "                      param_grid=param_grid,\n",
    "                      scoring='accuracy',\n",
    "                      cv=10)\n",
    "\n",
    "# Lets search\n",
    "search.fit(X_train,\n",
    "           y_train)\n",
    "\n",
    "print(search.best_params_)\n",
    "print(search.best_score_)\n",
    "\n",
    "xGBoost = search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfaf3810",
   "metadata": {},
   "source": [
    "## Evaluation-Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "0c48e2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_func(model, X_train, X_test, y_train, y_test):\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_hat = model.predict(X_test)\n",
    "    \n",
    "    accuracy = metrics.accuracy_score(y_test,y_hat)\n",
    "    precision = metrics.precision_score(y_test,y_hat,average='macro')\n",
    "    recall = metrics.recall_score(y_test,y_hat,average='macro')\n",
    "    f1 = metrics.f1_score(y_test,y_hat,average='macro')\n",
    "    \n",
    "    return [accuracy,precision,recall,f1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "e51a1c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:57:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:57:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "models = [logisticRegressor,\n",
    "          kNearestNeighbors,\n",
    "          decisionTree,\n",
    "          supportVectorMachine,\n",
    "          randomForest,\n",
    "          xGBoost]\n",
    "\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "for model in models:\n",
    "    # calculate scores\n",
    "    [acc,pre,rec,f1] = evaluation_func(model, X_train, X_test, y_train, y_test)\n",
    "    # append to correct lists\n",
    "    accuracy_scores.append(acc)\n",
    "    precision_scores.append(pre)\n",
    "    recall_scores.append(rec)\n",
    "    f1_scores.append(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "4a95d7b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels = ['Logistic Regression', 'kNN', 'Decision Tree', 'SVM', 'Random Forest', 'XGBoost']\n",
    "x = np.arange(len(labels))*2.5  # the label locations\n",
    "width = 0.50  # the width of the bars\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Form group bars\n",
    "rects1 = ax.bar(x - 0.75, accuracy_scores, width, label='Accuracy')\n",
    "rects2 = ax.bar(x - 0.25, precision_scores, width, label='Precision')\n",
    "rects3 = ax.bar(x + 0.25, recall_scores, width, label='Recall')\n",
    "rects4 = ax.bar(x + 0.75, f1_scores, width, label='F1')\n",
    "\n",
    "# Add some text for labels and custom x-axis tick labels\n",
    "ax.set_ylabel('Scores')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend()\n",
    "\n",
    "# Define bar labels properties\n",
    "ax.bar_label(rects1, fmt='%.2f', fontsize=9, rotation=90, padding=2)\n",
    "ax.bar_label(rects2, fmt='%.2f', fontsize=9, rotation=90, padding=2)\n",
    "ax.bar_label(rects3, fmt='%.2f', fontsize=9, rotation=90, padding=2)\n",
    "ax.bar_label(rects4, fmt='%.2f', fontsize=9, rotation=90, padding=2)\n",
    "\n",
    "fig.tight_layout(rect=(0,0,1.2,1.1))\n",
    "sns.despine()\n",
    "plt.show()\n",
    "\n",
    "plt.savefig('Scores_by_Algorythm.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff53b813",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "89a906b0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.095146</td>\n",
       "      <td>-0.270775</td>\n",
       "      <td>-0.160796</td>\n",
       "      <td>0.056499</td>\n",
       "      <td>0.263191</td>\n",
       "      <td>0.094090</td>\n",
       "      <td>0.193086</td>\n",
       "      <td>0.202750</td>\n",
       "      <td>0.450894</td>\n",
       "      <td>-0.162408</td>\n",
       "      <td>-0.019386</td>\n",
       "      <td>-0.189745</td>\n",
       "      <td>0.203501</td>\n",
       "      <td>-0.014376</td>\n",
       "      <td>0.328980</td>\n",
       "      <td>0.594006</td>\n",
       "      <td>-0.500987</td>\n",
       "      <td>0.458466</td>\n",
       "      <td>-0.144457</td>\n",
       "      <td>0.144544</td>\n",
       "      <td>-0.133649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.184818</td>\n",
       "      <td>-0.178934</td>\n",
       "      <td>-0.254737</td>\n",
       "      <td>0.034532</td>\n",
       "      <td>0.331010</td>\n",
       "      <td>0.116268</td>\n",
       "      <td>0.194603</td>\n",
       "      <td>-0.474799</td>\n",
       "      <td>0.634516</td>\n",
       "      <td>-0.238699</td>\n",
       "      <td>0.059973</td>\n",
       "      <td>-0.154528</td>\n",
       "      <td>0.179162</td>\n",
       "      <td>-0.017794</td>\n",
       "      <td>0.309899</td>\n",
       "      <td>0.595497</td>\n",
       "      <td>-0.511303</td>\n",
       "      <td>0.511923</td>\n",
       "      <td>-0.148456</td>\n",
       "      <td>0.084257</td>\n",
       "      <td>-0.133387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.237574</td>\n",
       "      <td>-0.115207</td>\n",
       "      <td>-0.200532</td>\n",
       "      <td>-0.011499</td>\n",
       "      <td>0.130076</td>\n",
       "      <td>-0.009226</td>\n",
       "      <td>0.228233</td>\n",
       "      <td>0.881630</td>\n",
       "      <td>0.304834</td>\n",
       "      <td>-0.129532</td>\n",
       "      <td>0.102924</td>\n",
       "      <td>-0.195364</td>\n",
       "      <td>0.180656</td>\n",
       "      <td>-0.004536</td>\n",
       "      <td>0.356172</td>\n",
       "      <td>0.546696</td>\n",
       "      <td>-0.459488</td>\n",
       "      <td>0.420419</td>\n",
       "      <td>-0.159465</td>\n",
       "      <td>0.086078</td>\n",
       "      <td>-0.159865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.285659</td>\n",
       "      <td>-0.150339</td>\n",
       "      <td>-0.289820</td>\n",
       "      <td>0.029470</td>\n",
       "      <td>0.353184</td>\n",
       "      <td>0.111596</td>\n",
       "      <td>0.173367</td>\n",
       "      <td>-0.434884</td>\n",
       "      <td>0.688644</td>\n",
       "      <td>-0.191000</td>\n",
       "      <td>0.428473</td>\n",
       "      <td>-0.128052</td>\n",
       "      <td>0.172569</td>\n",
       "      <td>-0.030816</td>\n",
       "      <td>0.113693</td>\n",
       "      <td>0.345776</td>\n",
       "      <td>-0.362756</td>\n",
       "      <td>1.135812</td>\n",
       "      <td>-0.271234</td>\n",
       "      <td>0.084436</td>\n",
       "      <td>-0.036880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.345141</td>\n",
       "      <td>-0.075935</td>\n",
       "      <td>-0.225586</td>\n",
       "      <td>-0.029478</td>\n",
       "      <td>0.132346</td>\n",
       "      <td>-0.027542</td>\n",
       "      <td>0.216382</td>\n",
       "      <td>0.928776</td>\n",
       "      <td>0.366435</td>\n",
       "      <td>-0.107082</td>\n",
       "      <td>0.489098</td>\n",
       "      <td>-0.160765</td>\n",
       "      <td>0.157516</td>\n",
       "      <td>-0.016043</td>\n",
       "      <td>0.160051</td>\n",
       "      <td>0.284551</td>\n",
       "      <td>-0.303923</td>\n",
       "      <td>1.043480</td>\n",
       "      <td>-0.284147</td>\n",
       "      <td>0.077171</td>\n",
       "      <td>-0.083043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011</th>\n",
       "      <td>-0.049534</td>\n",
       "      <td>0.163366</td>\n",
       "      <td>0.560462</td>\n",
       "      <td>0.155557</td>\n",
       "      <td>0.536468</td>\n",
       "      <td>0.171218</td>\n",
       "      <td>-0.005066</td>\n",
       "      <td>-0.043865</td>\n",
       "      <td>-0.171883</td>\n",
       "      <td>0.555886</td>\n",
       "      <td>0.059725</td>\n",
       "      <td>0.391102</td>\n",
       "      <td>-0.119554</td>\n",
       "      <td>-0.015795</td>\n",
       "      <td>0.044403</td>\n",
       "      <td>-0.442672</td>\n",
       "      <td>-0.457955</td>\n",
       "      <td>-0.061588</td>\n",
       "      <td>0.066864</td>\n",
       "      <td>0.179926</td>\n",
       "      <td>-0.149922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012</th>\n",
       "      <td>-0.060819</td>\n",
       "      <td>0.145451</td>\n",
       "      <td>0.543632</td>\n",
       "      <td>0.177232</td>\n",
       "      <td>0.569866</td>\n",
       "      <td>0.194113</td>\n",
       "      <td>-0.020814</td>\n",
       "      <td>-0.056000</td>\n",
       "      <td>-0.184423</td>\n",
       "      <td>0.598255</td>\n",
       "      <td>0.030067</td>\n",
       "      <td>0.377471</td>\n",
       "      <td>-0.091788</td>\n",
       "      <td>-0.018338</td>\n",
       "      <td>0.044259</td>\n",
       "      <td>-0.421826</td>\n",
       "      <td>-0.469732</td>\n",
       "      <td>-0.060198</td>\n",
       "      <td>0.070058</td>\n",
       "      <td>0.195173</td>\n",
       "      <td>-0.116890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013</th>\n",
       "      <td>-0.054619</td>\n",
       "      <td>0.155293</td>\n",
       "      <td>0.552878</td>\n",
       "      <td>0.165324</td>\n",
       "      <td>0.551518</td>\n",
       "      <td>0.181535</td>\n",
       "      <td>-0.012163</td>\n",
       "      <td>-0.049333</td>\n",
       "      <td>-0.177534</td>\n",
       "      <td>0.574979</td>\n",
       "      <td>0.046361</td>\n",
       "      <td>0.384960</td>\n",
       "      <td>-0.107042</td>\n",
       "      <td>-0.016941</td>\n",
       "      <td>0.044338</td>\n",
       "      <td>-0.433278</td>\n",
       "      <td>-0.463262</td>\n",
       "      <td>-0.060962</td>\n",
       "      <td>0.068303</td>\n",
       "      <td>0.186797</td>\n",
       "      <td>-0.135037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014</th>\n",
       "      <td>-0.064917</td>\n",
       "      <td>0.138945</td>\n",
       "      <td>0.537521</td>\n",
       "      <td>0.185104</td>\n",
       "      <td>0.581995</td>\n",
       "      <td>0.202427</td>\n",
       "      <td>-0.026533</td>\n",
       "      <td>-0.060406</td>\n",
       "      <td>-0.188976</td>\n",
       "      <td>0.613642</td>\n",
       "      <td>0.019297</td>\n",
       "      <td>0.372522</td>\n",
       "      <td>-0.081705</td>\n",
       "      <td>-0.019262</td>\n",
       "      <td>0.044207</td>\n",
       "      <td>-0.414255</td>\n",
       "      <td>-0.474009</td>\n",
       "      <td>-0.059694</td>\n",
       "      <td>0.071219</td>\n",
       "      <td>0.200710</td>\n",
       "      <td>-0.104895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015</th>\n",
       "      <td>-0.064077</td>\n",
       "      <td>0.140280</td>\n",
       "      <td>0.538774</td>\n",
       "      <td>0.183489</td>\n",
       "      <td>0.579507</td>\n",
       "      <td>0.200721</td>\n",
       "      <td>-0.025360</td>\n",
       "      <td>-0.059502</td>\n",
       "      <td>-0.188042</td>\n",
       "      <td>0.610485</td>\n",
       "      <td>0.021506</td>\n",
       "      <td>0.373537</td>\n",
       "      <td>-0.083773</td>\n",
       "      <td>-0.019072</td>\n",
       "      <td>0.044217</td>\n",
       "      <td>-0.415808</td>\n",
       "      <td>-0.473131</td>\n",
       "      <td>-0.059797</td>\n",
       "      <td>0.070981</td>\n",
       "      <td>0.199574</td>\n",
       "      <td>-0.107355</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1472 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6   \\\n",
       "0    -0.095146 -0.270775 -0.160796  0.056499  0.263191  0.094090  0.193086   \n",
       "1     0.184818 -0.178934 -0.254737  0.034532  0.331010  0.116268  0.194603   \n",
       "2     0.237574 -0.115207 -0.200532 -0.011499  0.130076 -0.009226  0.228233   \n",
       "3     0.285659 -0.150339 -0.289820  0.029470  0.353184  0.111596  0.173367   \n",
       "4     0.345141 -0.075935 -0.225586 -0.029478  0.132346 -0.027542  0.216382   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "2011 -0.049534  0.163366  0.560462  0.155557  0.536468  0.171218 -0.005066   \n",
       "2012 -0.060819  0.145451  0.543632  0.177232  0.569866  0.194113 -0.020814   \n",
       "2013 -0.054619  0.155293  0.552878  0.165324  0.551518  0.181535 -0.012163   \n",
       "2014 -0.064917  0.138945  0.537521  0.185104  0.581995  0.202427 -0.026533   \n",
       "2015 -0.064077  0.140280  0.538774  0.183489  0.579507  0.200721 -0.025360   \n",
       "\n",
       "            7         8         9         10        11        12        13  \\\n",
       "0     0.202750  0.450894 -0.162408 -0.019386 -0.189745  0.203501 -0.014376   \n",
       "1    -0.474799  0.634516 -0.238699  0.059973 -0.154528  0.179162 -0.017794   \n",
       "2     0.881630  0.304834 -0.129532  0.102924 -0.195364  0.180656 -0.004536   \n",
       "3    -0.434884  0.688644 -0.191000  0.428473 -0.128052  0.172569 -0.030816   \n",
       "4     0.928776  0.366435 -0.107082  0.489098 -0.160765  0.157516 -0.016043   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "2011 -0.043865 -0.171883  0.555886  0.059725  0.391102 -0.119554 -0.015795   \n",
       "2012 -0.056000 -0.184423  0.598255  0.030067  0.377471 -0.091788 -0.018338   \n",
       "2013 -0.049333 -0.177534  0.574979  0.046361  0.384960 -0.107042 -0.016941   \n",
       "2014 -0.060406 -0.188976  0.613642  0.019297  0.372522 -0.081705 -0.019262   \n",
       "2015 -0.059502 -0.188042  0.610485  0.021506  0.373537 -0.083773 -0.019072   \n",
       "\n",
       "            14        15        16        17        18        19        20  \n",
       "0     0.328980  0.594006 -0.500987  0.458466 -0.144457  0.144544 -0.133649  \n",
       "1     0.309899  0.595497 -0.511303  0.511923 -0.148456  0.084257 -0.133387  \n",
       "2     0.356172  0.546696 -0.459488  0.420419 -0.159465  0.086078 -0.159865  \n",
       "3     0.113693  0.345776 -0.362756  1.135812 -0.271234  0.084436 -0.036880  \n",
       "4     0.160051  0.284551 -0.303923  1.043480 -0.284147  0.077171 -0.083043  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "2011  0.044403 -0.442672 -0.457955 -0.061588  0.066864  0.179926 -0.149922  \n",
       "2012  0.044259 -0.421826 -0.469732 -0.060198  0.070058  0.195173 -0.116890  \n",
       "2013  0.044338 -0.433278 -0.463262 -0.060962  0.068303  0.186797 -0.135037  \n",
       "2014  0.044207 -0.414255 -0.474009 -0.059694  0.071219  0.200710 -0.104895  \n",
       "2015  0.044217 -0.415808 -0.473131 -0.059797  0.070981  0.199574 -0.107355  \n",
       "\n",
       "[1472 rows x 21 columns]"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "7f9d63a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       22\n",
       "1       22\n",
       "2       14\n",
       "3        9\n",
       "4       14\n",
       "        ..\n",
       "2011    23\n",
       "2012    23\n",
       "2013    23\n",
       "2014    23\n",
       "2015    23\n",
       "Name: Outcome, Length: 1472, dtype: int64"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "supportVectorMachine.fit(X_train, y_train)\n",
    "\n",
    "y_hat = model.predict(X_pred)\n",
    "y_hat = pd.Series(y_hat, name='Outcome', index=X_pred.index) # Convert to pd.Series with correct index\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d270614",
   "metadata": {},
   "source": [
    "### Merge_All_Three_SubDatasets_Back_to_One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "ae0f02ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vaw_train: (429, 22)\n",
      "vaw_test: (109, 22)\n",
      "vaw_pred:  (1472, 22)\n"
     ]
    }
   ],
   "source": [
    "# Concat horisontaly\n",
    "vaw_train = pd.concat([X_train, y_train], axis=1)\n",
    "vaw_test = pd.concat([X_test, y_test], axis=1)\n",
    "vaw_pred = pd.concat([X_pred, y_hat], axis=1)\n",
    "\n",
    "# Shape of dataset\n",
    "print('vaw_train: ' + str(vaw_train.shape))\n",
    "print('vaw_test: ' + str(vaw_test.shape))\n",
    "print('vaw_pred:  '  + str(vaw_pred.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "a363f47f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>P.Comp_01</th>\n",
       "      <th>P.Comp_02</th>\n",
       "      <th>P.Comp_03</th>\n",
       "      <th>P.Comp_04</th>\n",
       "      <th>P.Comp_05</th>\n",
       "      <th>P.Comp_06</th>\n",
       "      <th>P.Comp_07</th>\n",
       "      <th>P.Comp_08</th>\n",
       "      <th>P.Comp_09</th>\n",
       "      <th>P.Comp_10</th>\n",
       "      <th>P.Comp_11</th>\n",
       "      <th>P.Comp_12</th>\n",
       "      <th>P.Comp_13</th>\n",
       "      <th>P.Comp_14</th>\n",
       "      <th>P.Comp_15</th>\n",
       "      <th>P.Comp_16</th>\n",
       "      <th>P.Comp_17</th>\n",
       "      <th>P.Comp_18</th>\n",
       "      <th>P.Comp_19</th>\n",
       "      <th>P.Comp_20</th>\n",
       "      <th>P.Comp_21</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.095146</td>\n",
       "      <td>-0.270775</td>\n",
       "      <td>-0.160796</td>\n",
       "      <td>0.056499</td>\n",
       "      <td>0.263191</td>\n",
       "      <td>0.094090</td>\n",
       "      <td>0.193086</td>\n",
       "      <td>0.202750</td>\n",
       "      <td>0.450894</td>\n",
       "      <td>-0.162408</td>\n",
       "      <td>-0.019386</td>\n",
       "      <td>-0.189745</td>\n",
       "      <td>0.203501</td>\n",
       "      <td>-0.014376</td>\n",
       "      <td>0.328980</td>\n",
       "      <td>0.594006</td>\n",
       "      <td>-0.500987</td>\n",
       "      <td>0.458466</td>\n",
       "      <td>-0.144457</td>\n",
       "      <td>0.144544</td>\n",
       "      <td>-0.133649</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.184818</td>\n",
       "      <td>-0.178934</td>\n",
       "      <td>-0.254737</td>\n",
       "      <td>0.034532</td>\n",
       "      <td>0.331010</td>\n",
       "      <td>0.116268</td>\n",
       "      <td>0.194603</td>\n",
       "      <td>-0.474799</td>\n",
       "      <td>0.634516</td>\n",
       "      <td>-0.238699</td>\n",
       "      <td>0.059973</td>\n",
       "      <td>-0.154528</td>\n",
       "      <td>0.179162</td>\n",
       "      <td>-0.017794</td>\n",
       "      <td>0.309899</td>\n",
       "      <td>0.595497</td>\n",
       "      <td>-0.511303</td>\n",
       "      <td>0.511923</td>\n",
       "      <td>-0.148456</td>\n",
       "      <td>0.084257</td>\n",
       "      <td>-0.133387</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.237574</td>\n",
       "      <td>-0.115207</td>\n",
       "      <td>-0.200532</td>\n",
       "      <td>-0.011499</td>\n",
       "      <td>0.130076</td>\n",
       "      <td>-0.009226</td>\n",
       "      <td>0.228233</td>\n",
       "      <td>0.881630</td>\n",
       "      <td>0.304834</td>\n",
       "      <td>-0.129532</td>\n",
       "      <td>0.102924</td>\n",
       "      <td>-0.195364</td>\n",
       "      <td>0.180656</td>\n",
       "      <td>-0.004536</td>\n",
       "      <td>0.356172</td>\n",
       "      <td>0.546696</td>\n",
       "      <td>-0.459488</td>\n",
       "      <td>0.420419</td>\n",
       "      <td>-0.159465</td>\n",
       "      <td>0.086078</td>\n",
       "      <td>-0.159865</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.285659</td>\n",
       "      <td>-0.150339</td>\n",
       "      <td>-0.289820</td>\n",
       "      <td>0.029470</td>\n",
       "      <td>0.353184</td>\n",
       "      <td>0.111596</td>\n",
       "      <td>0.173367</td>\n",
       "      <td>-0.434884</td>\n",
       "      <td>0.688644</td>\n",
       "      <td>-0.191000</td>\n",
       "      <td>0.428473</td>\n",
       "      <td>-0.128052</td>\n",
       "      <td>0.172569</td>\n",
       "      <td>-0.030816</td>\n",
       "      <td>0.113693</td>\n",
       "      <td>0.345776</td>\n",
       "      <td>-0.362756</td>\n",
       "      <td>1.135812</td>\n",
       "      <td>-0.271234</td>\n",
       "      <td>0.084436</td>\n",
       "      <td>-0.036880</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.345141</td>\n",
       "      <td>-0.075935</td>\n",
       "      <td>-0.225586</td>\n",
       "      <td>-0.029478</td>\n",
       "      <td>0.132346</td>\n",
       "      <td>-0.027542</td>\n",
       "      <td>0.216382</td>\n",
       "      <td>0.928776</td>\n",
       "      <td>0.366435</td>\n",
       "      <td>-0.107082</td>\n",
       "      <td>0.489098</td>\n",
       "      <td>-0.160765</td>\n",
       "      <td>0.157516</td>\n",
       "      <td>-0.016043</td>\n",
       "      <td>0.160051</td>\n",
       "      <td>0.284551</td>\n",
       "      <td>-0.303923</td>\n",
       "      <td>1.043480</td>\n",
       "      <td>-0.284147</td>\n",
       "      <td>0.077171</td>\n",
       "      <td>-0.083043</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011</th>\n",
       "      <td>-0.049534</td>\n",
       "      <td>0.163366</td>\n",
       "      <td>0.560462</td>\n",
       "      <td>0.155557</td>\n",
       "      <td>0.536468</td>\n",
       "      <td>0.171218</td>\n",
       "      <td>-0.005066</td>\n",
       "      <td>-0.043865</td>\n",
       "      <td>-0.171883</td>\n",
       "      <td>0.555886</td>\n",
       "      <td>0.059725</td>\n",
       "      <td>0.391102</td>\n",
       "      <td>-0.119554</td>\n",
       "      <td>-0.015795</td>\n",
       "      <td>0.044403</td>\n",
       "      <td>-0.442672</td>\n",
       "      <td>-0.457955</td>\n",
       "      <td>-0.061588</td>\n",
       "      <td>0.066864</td>\n",
       "      <td>0.179926</td>\n",
       "      <td>-0.149922</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012</th>\n",
       "      <td>-0.060819</td>\n",
       "      <td>0.145451</td>\n",
       "      <td>0.543632</td>\n",
       "      <td>0.177232</td>\n",
       "      <td>0.569866</td>\n",
       "      <td>0.194113</td>\n",
       "      <td>-0.020814</td>\n",
       "      <td>-0.056000</td>\n",
       "      <td>-0.184423</td>\n",
       "      <td>0.598255</td>\n",
       "      <td>0.030067</td>\n",
       "      <td>0.377471</td>\n",
       "      <td>-0.091788</td>\n",
       "      <td>-0.018338</td>\n",
       "      <td>0.044259</td>\n",
       "      <td>-0.421826</td>\n",
       "      <td>-0.469732</td>\n",
       "      <td>-0.060198</td>\n",
       "      <td>0.070058</td>\n",
       "      <td>0.195173</td>\n",
       "      <td>-0.116890</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013</th>\n",
       "      <td>-0.054619</td>\n",
       "      <td>0.155293</td>\n",
       "      <td>0.552878</td>\n",
       "      <td>0.165324</td>\n",
       "      <td>0.551518</td>\n",
       "      <td>0.181535</td>\n",
       "      <td>-0.012163</td>\n",
       "      <td>-0.049333</td>\n",
       "      <td>-0.177534</td>\n",
       "      <td>0.574979</td>\n",
       "      <td>0.046361</td>\n",
       "      <td>0.384960</td>\n",
       "      <td>-0.107042</td>\n",
       "      <td>-0.016941</td>\n",
       "      <td>0.044338</td>\n",
       "      <td>-0.433278</td>\n",
       "      <td>-0.463262</td>\n",
       "      <td>-0.060962</td>\n",
       "      <td>0.068303</td>\n",
       "      <td>0.186797</td>\n",
       "      <td>-0.135037</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014</th>\n",
       "      <td>-0.064917</td>\n",
       "      <td>0.138945</td>\n",
       "      <td>0.537521</td>\n",
       "      <td>0.185104</td>\n",
       "      <td>0.581995</td>\n",
       "      <td>0.202427</td>\n",
       "      <td>-0.026533</td>\n",
       "      <td>-0.060406</td>\n",
       "      <td>-0.188976</td>\n",
       "      <td>0.613642</td>\n",
       "      <td>0.019297</td>\n",
       "      <td>0.372522</td>\n",
       "      <td>-0.081705</td>\n",
       "      <td>-0.019262</td>\n",
       "      <td>0.044207</td>\n",
       "      <td>-0.414255</td>\n",
       "      <td>-0.474009</td>\n",
       "      <td>-0.059694</td>\n",
       "      <td>0.071219</td>\n",
       "      <td>0.200710</td>\n",
       "      <td>-0.104895</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015</th>\n",
       "      <td>-0.064077</td>\n",
       "      <td>0.140280</td>\n",
       "      <td>0.538774</td>\n",
       "      <td>0.183489</td>\n",
       "      <td>0.579507</td>\n",
       "      <td>0.200721</td>\n",
       "      <td>-0.025360</td>\n",
       "      <td>-0.059502</td>\n",
       "      <td>-0.188042</td>\n",
       "      <td>0.610485</td>\n",
       "      <td>0.021506</td>\n",
       "      <td>0.373537</td>\n",
       "      <td>-0.083773</td>\n",
       "      <td>-0.019072</td>\n",
       "      <td>0.044217</td>\n",
       "      <td>-0.415808</td>\n",
       "      <td>-0.473131</td>\n",
       "      <td>-0.059797</td>\n",
       "      <td>0.070981</td>\n",
       "      <td>0.199574</td>\n",
       "      <td>-0.107355</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2010 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      P.Comp_01  P.Comp_02  P.Comp_03  P.Comp_04  P.Comp_05  P.Comp_06  \\\n",
       "0     -0.095146  -0.270775  -0.160796   0.056499   0.263191   0.094090   \n",
       "1      0.184818  -0.178934  -0.254737   0.034532   0.331010   0.116268   \n",
       "2      0.237574  -0.115207  -0.200532  -0.011499   0.130076  -0.009226   \n",
       "3      0.285659  -0.150339  -0.289820   0.029470   0.353184   0.111596   \n",
       "4      0.345141  -0.075935  -0.225586  -0.029478   0.132346  -0.027542   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2011  -0.049534   0.163366   0.560462   0.155557   0.536468   0.171218   \n",
       "2012  -0.060819   0.145451   0.543632   0.177232   0.569866   0.194113   \n",
       "2013  -0.054619   0.155293   0.552878   0.165324   0.551518   0.181535   \n",
       "2014  -0.064917   0.138945   0.537521   0.185104   0.581995   0.202427   \n",
       "2015  -0.064077   0.140280   0.538774   0.183489   0.579507   0.200721   \n",
       "\n",
       "      P.Comp_07  P.Comp_08  P.Comp_09  P.Comp_10  P.Comp_11  P.Comp_12  \\\n",
       "0      0.193086   0.202750   0.450894  -0.162408  -0.019386  -0.189745   \n",
       "1      0.194603  -0.474799   0.634516  -0.238699   0.059973  -0.154528   \n",
       "2      0.228233   0.881630   0.304834  -0.129532   0.102924  -0.195364   \n",
       "3      0.173367  -0.434884   0.688644  -0.191000   0.428473  -0.128052   \n",
       "4      0.216382   0.928776   0.366435  -0.107082   0.489098  -0.160765   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2011  -0.005066  -0.043865  -0.171883   0.555886   0.059725   0.391102   \n",
       "2012  -0.020814  -0.056000  -0.184423   0.598255   0.030067   0.377471   \n",
       "2013  -0.012163  -0.049333  -0.177534   0.574979   0.046361   0.384960   \n",
       "2014  -0.026533  -0.060406  -0.188976   0.613642   0.019297   0.372522   \n",
       "2015  -0.025360  -0.059502  -0.188042   0.610485   0.021506   0.373537   \n",
       "\n",
       "      P.Comp_13  P.Comp_14  P.Comp_15  P.Comp_16  P.Comp_17  P.Comp_18  \\\n",
       "0      0.203501  -0.014376   0.328980   0.594006  -0.500987   0.458466   \n",
       "1      0.179162  -0.017794   0.309899   0.595497  -0.511303   0.511923   \n",
       "2      0.180656  -0.004536   0.356172   0.546696  -0.459488   0.420419   \n",
       "3      0.172569  -0.030816   0.113693   0.345776  -0.362756   1.135812   \n",
       "4      0.157516  -0.016043   0.160051   0.284551  -0.303923   1.043480   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2011  -0.119554  -0.015795   0.044403  -0.442672  -0.457955  -0.061588   \n",
       "2012  -0.091788  -0.018338   0.044259  -0.421826  -0.469732  -0.060198   \n",
       "2013  -0.107042  -0.016941   0.044338  -0.433278  -0.463262  -0.060962   \n",
       "2014  -0.081705  -0.019262   0.044207  -0.414255  -0.474009  -0.059694   \n",
       "2015  -0.083773  -0.019072   0.044217  -0.415808  -0.473131  -0.059797   \n",
       "\n",
       "      P.Comp_19  P.Comp_20  P.Comp_21  Outcome  \n",
       "0     -0.144457   0.144544  -0.133649       22  \n",
       "1     -0.148456   0.084257  -0.133387       22  \n",
       "2     -0.159465   0.086078  -0.159865       14  \n",
       "3     -0.271234   0.084436  -0.036880        9  \n",
       "4     -0.284147   0.077171  -0.083043       14  \n",
       "...         ...        ...        ...      ...  \n",
       "2011   0.066864   0.179926  -0.149922       23  \n",
       "2012   0.070058   0.195173  -0.116890       23  \n",
       "2013   0.068303   0.186797  -0.135037       23  \n",
       "2014   0.071219   0.200710  -0.104895       23  \n",
       "2015   0.070981   0.199574  -0.107355       23  \n",
       "\n",
       "[2010 rows x 22 columns]"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concat with respect to index (verticaly)\n",
    "vaw_v4 = pd.concat([vaw_train, vaw_test, vaw_pred], axis=0).sort_index()\n",
    "\n",
    "# Change the col names (P.Comp = Principal Component)\n",
    "column_names = []\n",
    "\n",
    "for i in range(21):\n",
    "    column_names.append(f'P.Comp_{(i+1):02d}')\n",
    "    \n",
    "column_names.append('Outcome')\n",
    "\n",
    "vaw_v4.columns = column_names\n",
    "\n",
    "# Final Dataset\n",
    "vaw_v4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4e0da2",
   "metadata": {},
   "source": [
    "### Save_New_Dataset_as_.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "4f24030e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vaw_v4.to_csv('datasets/New_VAW.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00203096",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
